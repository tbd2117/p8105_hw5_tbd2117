---
title: "Homework 5"
author: "Thiago de Araujo - UNI tbd2117"
output: github_document
---

```{r, message=FALSE}
library(tidyverse)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.color = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d

set.seed(1)
```

### Problem 1

Loading homicide data, adjusting `report_date` variable, and creating `city_state` variable

```{r, message = FALSE}
homicide = 
  read_csv("./data/homicide-data.csv") %>% 
  separate(reported_date, into = c("year", "month", "day"), sep = c(4, 6)) %>% 
  mutate(city_state = str_c(city, ", ", state)) %>% 
  filter(!city_state == "Tulsa, AL") # n = 1, probable input error
```

The **homicide** dataset has `r nrow(homicide)` rows and `r ncol(homicide)` columns with data on homicides in 50 large U.S. cities gathered by the _Washington Post_ from `r min(pull(homicide, year))` to `r max(pull(homicide, year))`.

Creating variables to count total and unsolved homicides.

```{r}
homicide_city = 
  homicide %>% 
    group_by(city_state) %>%
    summarise(
      total_homicides = n(),
      unsolved_homicides = 
        sum(
          str_count(disposition, "Closed without arrest"), 
          str_count(disposition, "Open/No arrest")
        )
    )
```

Baltimore, MD estimated proportion (95%CI) of unsolved homicides:

```{r}
prop.test(
  homicide_city %>% filter(city_state == "Baltimore, MD") %>% pull(unsolved_homicides),
   homicide_city %>% filter(city_state == "Baltimore, MD") %>% pull(total_homicides)
) %>% 
  broom::tidy() %>% 
  select(estimate, conf.low, conf.high)
```

Iterating for all cities:

```{r}
city_results = 
homicide_city %>% 
  mutate(
    prop_test = map2(.x = unsolved_homicides, .y = total_homicides, ~prop.test(x = .x, n = .y)),
    tidy_test = map(.x = prop_test, ~broom::tidy(.x))
  ) %>% 
  unnest(tidy_test) %>% 
  select(city_state, estimate, conf.low, conf.high)
```

Plotting estimates and CIs for each city:

```{r}
city_results %>% 
  mutate(city_state = fct_reorder(city_state, desc(estimate))) %>% 
  ggplot(aes(x = city_state, y = estimate))+
  geom_point()+
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high))+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))+
  labs(
    x = "City, State",
    y = "Estimated Proportion of \n Unsolved Homicides (95%CI)"
  )
```

## Problem 2

Creating tidy dataset:

```{r, message = FALSE}
long_df = 
  tibble(
    path = list.files("./data/long_data")
  ) %>% 
  mutate(path = str_c("./data/long_data/",path),
  data = map(path, read_csv)) %>% 
  unnest(data) %>% 
  mutate(
    arm = case_when(
             str_detect(path, "con") ~ "control",
             str_detect(path, "exp") ~ "experimental"
          ),
    id = str_extract(path, "[0-9][0-9]")
  ) %>% 
  select(id, arm, week_1:week_8) %>% 
  pivot_longer(
    week_1:week_8, 
    names_to = "week",
    values_to = "observation"
  ) %>% 
  mutate(
    week = as.numeric(str_sub(week, -1))
  )
```

Spaghetti plot:

```{r}
long_df %>% 
  group_by(id, arm) %>% 
  ggplot(aes(x = week, y = observation, color = id))+
  geom_line()+
  facet_grid(. ~ arm)
```

Observations in the _experimental_ arm seem to increase over time while observations in the _control_ arm stay roughly the same.

## Problem 3

Estimated mean + p-value for t-tests for each _mu_

```{r}
est_p_t = function(x) {
  
  sim_data = 
  tibble(
    tibble(norm = rerun(5000, rnorm(30, mean = x, sd = 5)))
  ) %>% 
      mutate(
        sim_result = map(.x = norm, ~t.test(.x, mu = 0)),
        tidy_result = map(.x = sim_result, ~broom::tidy(.x))
      ) %>% 
      select(-sim_result) %>% 
      unnest(tidy_result) %>% 
      select(estimate, p.value)
  
}

output = 
  map_df(c(0:6), est_p_t, .id = "average") %>% 
  mutate(average = as.numeric(average) - 1)
```

Proportion of rejection of null hypothesis...

```{r}
output_reject = 
  output %>% 
    group_by(average) %>% 
    mutate(
      rejection = case_when(
                        p.value < 0.05  ~ 1,
                        p.value >= 0.05 ~ 0
                  )
    )
```

#### Plot of Power vs. _mu_

```{r}
output_reject %>%
  group_by(average) %>% 
    summarize(
      total = n(),
      rejected = sum(rejection)
    ) %>% 
  ggplot(aes(x = average, y = rejected / total))+
  geom_point()+
  labs(
    x = "True Mean",
    y = "Proportion of time the null was rejected \n(Power)"
  )
```

As effect size increases (bigger difference between tru mean and 0 in this case) power increases.

##### Plot of average estimates of ^μ and by μ
 

```{r}
plot_overlay =   
  output_reject %>%
  filter(rejection == 1) %>% 
    mutate(
      average_est_reject = mean(estimate)
    )

output_reject %>%
  group_by(average) %>% 
    mutate(
      average_est = mean(estimate)
    )  %>% 
  ggplot(aes(x = average, y = average_est))+
  geom_point(color = "red")+
  geom_point(data = plot_overlay, aes(x = average, y = average_est_reject), color = "blue")+
  labs(
    x = "True Mean",
    y = "Average estimate of True Mean",
    caption = "RED = Average estimate of μ \n BLUE = Average estimate of μ only in samples for which the null was rejected"
  )
```

The sample average of μ̂ across tests for which the null is rejected

